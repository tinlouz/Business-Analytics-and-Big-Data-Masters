{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Martin Louzada\n",
    "\n",
    "#### February 24th 2019\n",
    "\n",
    "## Text classifier\n",
    "### Train a text classifier that categorizes movie reviews in \"positive\" and \"negative\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " !jupytext --check Black C:\\Users\\jalouzada\\MBD\\Python\\Assignment Sentiment Analysis\\2_TextClassifier_Martin_Louzada.ipynb -o C:\\Users\\jalouzada\\new_2_TextClassifier_Martin_Louzada.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jalouzada\n"
     ]
    }
   ],
   "source": [
    "cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages and libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import LeaveOneOut, StratifiedKFold, StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all the text files from aclImdb/train/pos/ and aclImdb/train/neg/ into a pandas DataFrame called train with two columns: review (the text itself) and sentiment (positive or negative). Do the same thing with aclImdb/test and save it to a different DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jalouzada\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train DataFrame\n",
    "path1 = \"C:/Users/jalouzada/Google Drive/MDB/Python/Assignment Sentiment Analysis/aclImdb/train/pos/*.txt\"\n",
    "\n",
    "files1 = glob.glob(path1)\n",
    "\n",
    "df1 = pd.concat([pd.read_table(f, header=None) for f in files1])\n",
    "df1[\"sentiment\"] = \"Positive\"\n",
    "\n",
    "path2 = \"C:/Users/jalouzada/Google Drive/MDB/Python/Assignment Sentiment Analysis/aclImdb/train/neg/*.txt\"\n",
    "\n",
    "files2 = glob.glob(path2)\n",
    "\n",
    "df2 = pd.concat([pd.read_table(f, header=None) for f in files2])\n",
    "df2[\"sentiment\"] = \"Negative\"\n",
    "\n",
    "reviews_train = pd.concat([df1, df2])\n",
    "reviews_train = reviews_train.drop(columns=[1, 2, 3, 4, 5, 6, 7])\n",
    "reviews_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jalouzada\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test DataFrame\n",
    "path1 = \"C:/Users/jalouzada/Google Drive/MDB/Python/Assignment Sentiment Analysis/aclImdb/test/pos/*.txt\"\n",
    "\n",
    "files1 = glob.glob(path1)\n",
    "\n",
    "df1 = pd.concat([pd.read_table(f, header=None) for f in files1])\n",
    "df1[\"sentiment\"] = \"Positive\"\n",
    "\n",
    "path2 = \"C:/Users/jalouzada/Google Drive/MDB/Python/Assignment Sentiment Analysis/aclImdb/test/neg/*.txt\"\n",
    "\n",
    "files2 = glob.glob(path2)\n",
    "\n",
    "df2 = pd.concat([pd.read_table(f, header=None) for f in files2])\n",
    "df2[\"sentiment\"] = \"Negative\"\n",
    "\n",
    "reviews_test = pd.concat([df1, df2])\n",
    "reviews_test = reviews_test.drop(columns=[1, 2, 3, 4, 5, 6, 7, 8])\n",
    "reviews_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the text using CountVectorizer or TfidfVectorizer from scikit-learn, adding a preprocessor that removes spurious \"br/\" tags from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(\n",
    "    stop_words={\n",
    "        \"of\",\n",
    "        \"the\",\n",
    "        \"film\",\n",
    "        \"I\",\n",
    "        \"movie\",\n",
    "        \"think\",\n",
    "        \"plot\",\n",
    "        \"a\",\n",
    "        \"able\",\n",
    "        \"about\",\n",
    "        \"above\",\n",
    "        \"abst\",\n",
    "        \"accordance\",\n",
    "        \"according\",\n",
    "        \"accordingly\",\n",
    "        \"across\",\n",
    "        \"act\",\n",
    "        \"actually\",\n",
    "        \"added\",\n",
    "        \"adj\",\n",
    "        \"affected\",\n",
    "        \"affecting\",\n",
    "        \"affects\",\n",
    "        \"after\",\n",
    "        \"afterwards\",\n",
    "        \"again\",\n",
    "        \"against\",\n",
    "        \"ah\",\n",
    "        \"all\",\n",
    "        \"almost\",\n",
    "        \"alone\",\n",
    "        \"along\",\n",
    "        \"already\",\n",
    "        \"also\",\n",
    "        \"although\",\n",
    "        \"always\",\n",
    "        \"am\",\n",
    "        \"among\",\n",
    "        \"amongst\",\n",
    "        \"an\",\n",
    "        \"and\",\n",
    "        \"announce\",\n",
    "        \"another\",\n",
    "        \"any\",\n",
    "        \"anybody\",\n",
    "        \"anyhow\",\n",
    "        \"anymore\",\n",
    "        \"anyone\",\n",
    "        \"anything\",\n",
    "        \"anyway\",\n",
    "        \"anyways\",\n",
    "        \"anywhere\",\n",
    "        \"apparently\",\n",
    "        \"approximately\",\n",
    "        \"are\",\n",
    "        \"aren\",\n",
    "        \"arent\",\n",
    "        \"arise\",\n",
    "        \"around\",\n",
    "        \"as\",\n",
    "        \"aside\",\n",
    "        \"ask\",\n",
    "        \"asking\",\n",
    "        \"at\",\n",
    "        \"auth\",\n",
    "        \"available\",\n",
    "        \"away\",\n",
    "        \"awfully\",\n",
    "        \"b\",\n",
    "        \"back\",\n",
    "        \"be\",\n",
    "        \"became\",\n",
    "        \"because\",\n",
    "        \"become\",\n",
    "        \"becomes\",\n",
    "        \"becoming\",\n",
    "        \"been\",\n",
    "        \"before\",\n",
    "        \"beforehand\",\n",
    "        \"begin\",\n",
    "        \"beginning\",\n",
    "        \"beginnings\",\n",
    "        \"begins\",\n",
    "        \"behind\",\n",
    "        \"being\",\n",
    "        \"believe\",\n",
    "        \"below\",\n",
    "        \"beside\",\n",
    "        \"besides\",\n",
    "        \"between\",\n",
    "        \"beyond\",\n",
    "        \"biol\",\n",
    "        \"both\",\n",
    "        \"brief\",\n",
    "        \"briefly\",\n",
    "        \"but\",\n",
    "        \"by\",\n",
    "        \"c\",\n",
    "        \"ca\",\n",
    "        \"came\",\n",
    "        \"can\",\n",
    "        \"cannot\",\n",
    "        \"cant\",\n",
    "        \"cause\",\n",
    "        \"causes\",\n",
    "        \"certain\",\n",
    "        \"certainly\",\n",
    "        \"co\",\n",
    "        \"com\",\n",
    "        \"come\",\n",
    "        \"comes\",\n",
    "        \"contain\",\n",
    "        \"containing\",\n",
    "        \"contains\",\n",
    "        \"could\",\n",
    "        \"couldnt\",\n",
    "        \"d\",\n",
    "        \"date\",\n",
    "        \"did\",\n",
    "        \"didnt\",\n",
    "        \"different\",\n",
    "        \"do\",\n",
    "        \"does\",\n",
    "        \"doesnt\",\n",
    "        \"doing\",\n",
    "        \"done\",\n",
    "        \"dont\",\n",
    "        \"down\",\n",
    "        \"downwards\",\n",
    "        \"due\",\n",
    "        \"during\",\n",
    "        \"e\",\n",
    "        \"each\",\n",
    "        \"ed\",\n",
    "        \"edu\",\n",
    "        \"effect\",\n",
    "        \"eg\",\n",
    "        \"eight\",\n",
    "        \"eighty\",\n",
    "        \"either\",\n",
    "        \"else\",\n",
    "        \"elsewhere\",\n",
    "        \"end\",\n",
    "        \"ending\",\n",
    "        \"enough\",\n",
    "        \"especially\",\n",
    "        \"et\",\n",
    "        \"et-al\",\n",
    "        \"etc\",\n",
    "        \"even\",\n",
    "        \"ever\",\n",
    "        \"every\",\n",
    "        \"everybody\",\n",
    "        \"everyone\",\n",
    "        \"everything\",\n",
    "        \"everywhere\",\n",
    "        \"ex\",\n",
    "        \"except\",\n",
    "        \"f\",\n",
    "        \"far\",\n",
    "        \"few\",\n",
    "        \"ff\",\n",
    "        \"fifth\",\n",
    "        \"first\",\n",
    "        \"five\",\n",
    "        \"fix\",\n",
    "        \"followed\",\n",
    "        \"following\",\n",
    "        \"follows\",\n",
    "        \"for\",\n",
    "        \"former\",\n",
    "        \"formerly\",\n",
    "        \"forth\",\n",
    "        \"found\",\n",
    "        \"four\",\n",
    "        \"from\",\n",
    "        \"further\",\n",
    "        \"furthermore\",\n",
    "        \"g\",\n",
    "        \"gave\",\n",
    "        \"get\",\n",
    "        \"gets\",\n",
    "        \"getting\",\n",
    "        \"give\",\n",
    "        \"given\",\n",
    "        \"gives\",\n",
    "        \"giving\",\n",
    "        \"go\",\n",
    "        \"goes\",\n",
    "        \"gone\",\n",
    "        \"got\",\n",
    "        \"gotten\",\n",
    "        \"h\",\n",
    "        \"had\",\n",
    "        \"happens\",\n",
    "        \"hardly\",\n",
    "        \"has\",\n",
    "        \"hasnt\",\n",
    "        \"have\",\n",
    "        \"havent\",\n",
    "        \"having\",\n",
    "        \"he\",\n",
    "        \"hed\",\n",
    "        \"hence\",\n",
    "        \"her\",\n",
    "        \"here\",\n",
    "        \"hereafter\",\n",
    "        \"hereby\",\n",
    "        \"herein\",\n",
    "        \"heres\",\n",
    "        \"hereupon\",\n",
    "        \"hers\",\n",
    "        \"herself\",\n",
    "        \"hes\",\n",
    "        \"hi\",\n",
    "        \"hid\",\n",
    "        \"him\",\n",
    "        \"himself\",\n",
    "        \"his\",\n",
    "        \"hither\",\n",
    "        \"home\",\n",
    "        \"how\",\n",
    "        \"howbeit\",\n",
    "        \"however\",\n",
    "        \"hundred\",\n",
    "        \"i\",\n",
    "        \"id\",\n",
    "        \"ie\",\n",
    "        \"if\",\n",
    "        \"ill\",\n",
    "        \"im\",\n",
    "        \"immediate\",\n",
    "        \"immediately\",\n",
    "        \"importance\",\n",
    "        \"important\",\n",
    "        \"in\",\n",
    "        \"inc\",\n",
    "        \"indeed\",\n",
    "        \"index\",\n",
    "        \"information\",\n",
    "        \"instead\",\n",
    "        \"into\",\n",
    "        \"invention\",\n",
    "        \"inward\",\n",
    "        \"is\",\n",
    "        \"isnt\",\n",
    "        \"it\",\n",
    "        \"itd\",\n",
    "        \"itll\",\n",
    "        \"its\",\n",
    "        \"itself\",\n",
    "        \"ive\",\n",
    "        \"j\",\n",
    "        \"just\",\n",
    "        \"k\",\n",
    "        \"keep\",\n",
    "        \"keeps\",\n",
    "        \"kept\",\n",
    "        \"kg\",\n",
    "        \"km\",\n",
    "        \"know\",\n",
    "        \"known\",\n",
    "        \"knows\",\n",
    "        \"l\",\n",
    "        \"largely\",\n",
    "        \"last\",\n",
    "        \"lately\",\n",
    "        \"later\",\n",
    "        \"latter\",\n",
    "        \"latterly\",\n",
    "        \"least\",\n",
    "        \"less\",\n",
    "        \"lest\",\n",
    "        \"let\",\n",
    "        \"lets\",\n",
    "        \"like\",\n",
    "        \"liked\",\n",
    "        \"likely\",\n",
    "        \"line\",\n",
    "        \"little\",\n",
    "        \"ll\",\n",
    "        \"look\",\n",
    "        \"looking\",\n",
    "        \"looks\",\n",
    "        \"ltd\",\n",
    "        \"m\",\n",
    "        \"made\",\n",
    "        \"mainly\",\n",
    "        \"make\",\n",
    "        \"makes\",\n",
    "        \"many\",\n",
    "        \"may\",\n",
    "        \"maybe\",\n",
    "        \"me\",\n",
    "        \"mean\",\n",
    "        \"means\",\n",
    "        \"meantime\",\n",
    "        \"meanwhile\",\n",
    "        \"merely\",\n",
    "        \"mg\",\n",
    "        \"might\",\n",
    "        \"million\",\n",
    "        \"miss\",\n",
    "        \"ml\",\n",
    "        \"more\",\n",
    "        \"moreover\",\n",
    "        \"most\",\n",
    "        \"mostly\",\n",
    "        \"mr\",\n",
    "        \"mrs\",\n",
    "        \"much\",\n",
    "        \"mug\",\n",
    "        \"must\",\n",
    "        \"my\",\n",
    "        \"myself\",\n",
    "        \"n\",\n",
    "        \"na\",\n",
    "        \"name\",\n",
    "        \"namely\",\n",
    "        \"nay\",\n",
    "        \"nd\",\n",
    "        \"near\",\n",
    "        \"nearly\",\n",
    "        \"necessarily\",\n",
    "        \"necessary\",\n",
    "        \"need\",\n",
    "        \"needs\",\n",
    "        \"neither\",\n",
    "        \"never\",\n",
    "        \"nevertheless\",\n",
    "        \"new\",\n",
    "        \"next\",\n",
    "        \"nine\",\n",
    "        \"ninety\",\n",
    "        \"no\",\n",
    "        \"nobody\",\n",
    "        \"non\",\n",
    "        \"none\",\n",
    "        \"nonetheless\",\n",
    "        \"noone\",\n",
    "        \"nor\",\n",
    "        \"normally\",\n",
    "        \"nos\",\n",
    "        \"not\",\n",
    "        \"noted\",\n",
    "        \"nothing\",\n",
    "        \"now\",\n",
    "        \"nowhere\",\n",
    "        \"o\",\n",
    "        \"obtain\",\n",
    "        \"obtained\",\n",
    "        \"obviously\",\n",
    "        \"of\",\n",
    "        \"off\",\n",
    "        \"often\",\n",
    "        \"oh\",\n",
    "        \"ok\",\n",
    "        \"okay\",\n",
    "        \"old\",\n",
    "        \"omitted\",\n",
    "        \"on\",\n",
    "        \"once\",\n",
    "        \"one\",\n",
    "        \"ones\",\n",
    "        \"only\",\n",
    "        \"onto\",\n",
    "        \"or\",\n",
    "        \"ord\",\n",
    "        \"other\",\n",
    "        \"others\",\n",
    "        \"otherwise\",\n",
    "        \"ought\",\n",
    "        \"our\",\n",
    "        \"ours\",\n",
    "        \"ourselves\",\n",
    "        \"out\",\n",
    "        \"outside\",\n",
    "        \"over\",\n",
    "        \"overall\",\n",
    "        \"owing\",\n",
    "        \"own\",\n",
    "        \"p\",\n",
    "        \"page\",\n",
    "        \"pages\",\n",
    "        \"part\",\n",
    "        \"particular\",\n",
    "        \"particularly\",\n",
    "        \"past\",\n",
    "        \"per\",\n",
    "        \"perhaps\",\n",
    "        \"placed\",\n",
    "        \"please\",\n",
    "        \"plus\",\n",
    "        \"poorly\",\n",
    "        \"possible\",\n",
    "        \"possibly\",\n",
    "        \"potentially\",\n",
    "        \"pp\",\n",
    "        \"predominantly\",\n",
    "        \"present\",\n",
    "        \"previously\",\n",
    "        \"primarily\",\n",
    "        \"probably\",\n",
    "        \"promptly\",\n",
    "        \"proud\",\n",
    "        \"provides\",\n",
    "        \"put\",\n",
    "        \"q\",\n",
    "        \"que\",\n",
    "        \"quickly\",\n",
    "        \"quite\",\n",
    "        \"qv\",\n",
    "        \"r\",\n",
    "        \"ran\",\n",
    "        \"rather\",\n",
    "        \"rd\",\n",
    "        \"re\",\n",
    "        \"readily\",\n",
    "        \"really\",\n",
    "        \"recent\",\n",
    "        \"recently\",\n",
    "        \"ref\",\n",
    "        \"refs\",\n",
    "        \"regarding\",\n",
    "        \"regardless\",\n",
    "        \"regards\",\n",
    "        \"related\",\n",
    "        \"relatively\",\n",
    "        \"research\",\n",
    "        \"respectively\",\n",
    "        \"resulted\",\n",
    "        \"resulting\",\n",
    "        \"results\",\n",
    "        \"right\",\n",
    "        \"run\",\n",
    "        \"s\",\n",
    "        \"said\",\n",
    "        \"same\",\n",
    "        \"saw\",\n",
    "        \"say\",\n",
    "        \"saying\",\n",
    "        \"says\",\n",
    "        \"sec\",\n",
    "        \"section\",\n",
    "        \"see\",\n",
    "        \"seeing\",\n",
    "        \"seem\",\n",
    "        \"seemed\",\n",
    "        \"seeming\",\n",
    "        \"seems\",\n",
    "        \"seen\",\n",
    "        \"self\",\n",
    "        \"selves\",\n",
    "        \"sent\",\n",
    "        \"seven\",\n",
    "        \"several\",\n",
    "        \"shall\",\n",
    "        \"she\",\n",
    "        \"shed\",\n",
    "        \"shell\",\n",
    "        \"shes\",\n",
    "        \"should\",\n",
    "        \"shouldnt\",\n",
    "        \"show\",\n",
    "        \"showed\",\n",
    "        \"shown\",\n",
    "        \"showns\",\n",
    "        \"shows\",\n",
    "        \"significant\",\n",
    "        \"significantly\",\n",
    "        \"similar\",\n",
    "        \"similarly\",\n",
    "        \"since\",\n",
    "        \"six\",\n",
    "        \"slightly\",\n",
    "        \"so\",\n",
    "        \"some\",\n",
    "        \"somebody\",\n",
    "        \"somehow\",\n",
    "        \"someone\",\n",
    "        \"somethan\",\n",
    "        \"something\",\n",
    "        \"sometime\",\n",
    "        \"sometimes\",\n",
    "        \"somewhat\",\n",
    "        \"somewhere\",\n",
    "        \"soon\",\n",
    "        \"sorry\",\n",
    "        \"specifically\",\n",
    "        \"specified\",\n",
    "        \"specify\",\n",
    "        \"specifying\",\n",
    "        \"still\",\n",
    "        \"stop\",\n",
    "        \"strongly\",\n",
    "        \"sub\",\n",
    "        \"substantially\",\n",
    "        \"successfully\",\n",
    "        \"such\",\n",
    "        \"sufficiently\",\n",
    "        \"suggest\",\n",
    "        \"sup\",\n",
    "        \"sure\",\n",
    "        \"t\",\n",
    "        \"take\",\n",
    "        \"taken\",\n",
    "        \"taking\",\n",
    "        \"tell\",\n",
    "        \"tends\",\n",
    "        \"th\",\n",
    "        \"than\",\n",
    "        \"thank\",\n",
    "        \"thanks\",\n",
    "        \"thanx\",\n",
    "        \"that\",\n",
    "        \"thatll\",\n",
    "        \"thats\",\n",
    "        \"thatve\",\n",
    "        \"the\",\n",
    "        \"their\",\n",
    "        \"theirs\",\n",
    "        \"them\",\n",
    "        \"themselves\",\n",
    "        \"then\",\n",
    "        \"thence\",\n",
    "        \"there\",\n",
    "        \"thereafter\",\n",
    "        \"thereby\",\n",
    "        \"thered\",\n",
    "        \"therefore\",\n",
    "        \"therein\",\n",
    "        \"therell\",\n",
    "        \"thereof\",\n",
    "        \"therere\",\n",
    "        \"theres\",\n",
    "        \"thereto\",\n",
    "        \"thereupon\",\n",
    "        \"thereve\",\n",
    "        \"these\",\n",
    "        \"they\",\n",
    "        \"theyd\",\n",
    "        \"theyll\",\n",
    "        \"theyre\",\n",
    "        \"theyve\",\n",
    "        \"think\",\n",
    "        \"this\",\n",
    "        \"those\",\n",
    "        \"thou\",\n",
    "        \"though\",\n",
    "        \"thoughh\",\n",
    "        \"thousand\",\n",
    "        \"throug\",\n",
    "        \"through\",\n",
    "        \"throughout\",\n",
    "        \"thru\",\n",
    "        \"thus\",\n",
    "        \"til\",\n",
    "        \"tip\",\n",
    "        \"to\",\n",
    "        \"together\",\n",
    "        \"too\",\n",
    "        \"took\",\n",
    "        \"toward\",\n",
    "        \"towards\",\n",
    "        \"tried\",\n",
    "        \"tries\",\n",
    "        \"truly\",\n",
    "        \"try\",\n",
    "        \"trying\",\n",
    "        \"ts\",\n",
    "        \"twice\",\n",
    "        \"two\",\n",
    "        \"u\",\n",
    "        \"un\",\n",
    "        \"under\",\n",
    "        \"unfortunately\",\n",
    "        \"unless\",\n",
    "        \"unlike\",\n",
    "        \"unlikely\",\n",
    "        \"until\",\n",
    "        \"unto\",\n",
    "        \"up\",\n",
    "        \"upon\",\n",
    "        \"ups\",\n",
    "        \"us\",\n",
    "        \"use\",\n",
    "        \"used\",\n",
    "        \"useful\",\n",
    "        \"usefully\",\n",
    "        \"usefulness\",\n",
    "        \"uses\",\n",
    "        \"using\",\n",
    "        \"usually\",\n",
    "        \"v\",\n",
    "        \"value\",\n",
    "        \"various\",\n",
    "        \"ve\",\n",
    "        \"very\",\n",
    "        \"via\",\n",
    "        \"viz\",\n",
    "        \"vol\",\n",
    "        \"vols\",\n",
    "        \"vs\",\n",
    "        \"w\",\n",
    "        \"want\",\n",
    "        \"wants\",\n",
    "        \"was\",\n",
    "        \"wasnt\",\n",
    "        \"way\",\n",
    "        \"we\",\n",
    "        \"wed\",\n",
    "        \"welcome\",\n",
    "        \"well\",\n",
    "        \"went\",\n",
    "        \"were\",\n",
    "        \"werent\",\n",
    "        \"weve\",\n",
    "        \"what\",\n",
    "        \"whatever\",\n",
    "        \"whatll\",\n",
    "        \"whats\",\n",
    "        \"when\",\n",
    "        \"whence\",\n",
    "        \"whenever\",\n",
    "        \"where\",\n",
    "        \"whereafter\",\n",
    "        \"whereas\",\n",
    "        \"whereby\",\n",
    "        \"wherein\",\n",
    "        \"wheres\",\n",
    "        \"whereupon\",\n",
    "        \"wherever\",\n",
    "        \"whether\",\n",
    "        \"which\",\n",
    "        \"while\",\n",
    "        \"whim\",\n",
    "        \"whither\",\n",
    "        \"who\",\n",
    "        \"whod\",\n",
    "        \"whoever\",\n",
    "        \"whole\",\n",
    "        \"wholl\",\n",
    "        \"whom\",\n",
    "        \"whomever\",\n",
    "        \"whos\",\n",
    "        \"whose\",\n",
    "        \"why\",\n",
    "        \"widely\",\n",
    "        \"willing\",\n",
    "        \"wish\",\n",
    "        \"with\",\n",
    "        \"within\",\n",
    "        \"without\",\n",
    "        \"wont\",\n",
    "        \"words\",\n",
    "        \"world\",\n",
    "        \"would\",\n",
    "        \"wouldnt\",\n",
    "        \"www\",\n",
    "        \"x\",\n",
    "        \"y\",\n",
    "        \"yes\",\n",
    "        \"yet\",\n",
    "        \"you\",\n",
    "        \"youd\",\n",
    "        \"youll\",\n",
    "        \"your\",\n",
    "        \"youre\",\n",
    "        \"yours\",\n",
    "        \"yourself\",\n",
    "        \"yourselves\",\n",
    "        \"youve\",\n",
    "        \"z\",\n",
    "        \"zero\",\n",
    "    }\n",
    ")\n",
    "# sample to avoid MemoryError\n",
    "reviews_train_sample = reviews_train.sample(n=5000, random_state=1)\n",
    "X = pd.DataFrame(\n",
    "    vec.fit_transform(reviews_train_sample[0]).todense(),\n",
    "    columns=vec.get_feature_names(),\n",
    ")\n",
    "reviews_train_sample = reviews_train_sample.set_index(np.arange(5000))\n",
    "\n",
    "reviews_test_sample = reviews_test.sample(n=5000, random_state=1)\n",
    "Y = pd.DataFrame(\n",
    "    vec.fit_transform(reviews_test_sample[0]).todense(), columns=vec.get_feature_names()\n",
    ")\n",
    "reviews_test_sample = reviews_test_sample.set_index(np.arange(5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jalouzada\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.8424\n",
      "Accuracy for C=0.05: 0.8512\n",
      "Accuracy for C=0.25: 0.8528\n",
      "Accuracy for C=0.5: 0.8472\n",
      "Accuracy for C=1: 0.84\n"
     ]
    }
   ],
   "source": [
    "target = [\n",
    "    1 if reviews_train_sample.loc[i, \"sentiment\"] == \"Positive\" else 0\n",
    "    for i in range(5000)\n",
    "]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, target, train_size=0.75)\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "\n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print(\"Accuracy for C=%s: %s\" % (c, accuracy_score(y_val, lr.predict(X_val))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C= 0.25 gives us the best score so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.996\n"
     ]
    }
   ],
   "source": [
    "# train final model\n",
    "final_model = LogisticRegression(C=0.25)\n",
    "final_model.fit(X, target)\n",
    "print(\"Final Accuracy: %s\" % accuracy_score(target, final_model.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate the train and test data, and use GridSearchCV to optimize the C hyperparameter of the logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\"C\": [1e-05, 1e-04, 1e-03, 1e-02, 1e-01, 1, 10, 100, 1000, 10000]}\n",
    "\n",
    "logRegressionGrid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "\n",
    "# concatenate train and test\n",
    "final_df = pd.concat([X, Y])\n",
    "\n",
    "review_final_sample = pd.concat([reviews_train_sample, reviews_test_sample])\n",
    "review_final_sample = review_final_sample.set_index(np.arange(10000))\n",
    "\n",
    "final_target = [\n",
    "    1 if review_final_sample.loc[i, \"sentiment\"] == \"Positive\" else 0\n",
    "    for i in range(10000)\n",
    "]\n",
    "\n",
    "logRegressionGrid.fit(final_df, final_target)\n",
    "logRegressionGrid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
